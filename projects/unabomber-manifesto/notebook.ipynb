{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Unabomber Manifesto\n",
    "### CSC630: Rudd Fawcett\n",
    "\n",
    "\n",
    "Below, please find specifications about the development of my text project. The website [is live here.](http://csc630.rudd.io/projects/unabomber-manifesto/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the production of a single text-based visualization, this project focused on the mapping of the fifty most frequent words in the manifesto in a strip based fashion. The goals of this project were two fold:\n",
    "\n",
    "1. To develop a beautiful experience and website that accurately and interactively visualized the distribution of text.\n",
    "2. To develop a firm grasp and understanding of basic text analysis through the nltk Python library.\n",
    "\n",
    "Having just finished The Discovery Channel's documentary on the Unabomber, I was looking forward to starting this project.\n",
    "\n",
    "Overall, this project would not have been possible without the raw text of the manifesto as originally published in *The Washington Post*, and that was [distributed online](http://www.washingtonpost.com/wp-srv/national/longterm/unabomber/manifesto.text.htm). The manifesto, which includes over thirty-five thousand words, is broken up in sections and then paragraphs, in the style of a mid-1970s doctoral thesis paper.\n",
    "\n",
    "The visualization for this project is rather unconvential, and frankly something that I haven't seen anywhere else (so I may have invited something new?!). It maps the book horizontally, broken up and marked by sections and chapters, and then plots the fifty most frequent words across this horizontal and vertical plane. I would encourage you to check it out through the link above for a better idea of what I'm talking about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preparing Data for Use\n",
    "\n",
    "Preparing data for this project proved to be a huge pain and a waste of a bunch of time. Originally, I analyzed the text and broke it up by section > paragraph > sentences. This proved to be interesting as looking at pieces of the whole, but when putting the parts together, none of it lined up (text indicies for example). Therefore I opted out of that type of analysis and just ended up taking the data set head on.\n",
    "\n",
    "While analyzing the data, I had three different scripts: two that generated the indicies of paragraphs and sections based on regular expressions, and then another which tabulated the most frequent words throughout the manifesto. Using the three output files from these scripts, you can start to stitch together various layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Section Indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "punctuation = string.punctuation + '“”’.,'\n",
    "text = open('manifesto.txt').read()\n",
    "\n",
    "ch_tokenizer = RegexpTokenizer('^(?:[A-Z-‘’]+ ?)+?(?=\\n)')\n",
    "sections_by_title_indices = list(zip(ch_tokenizer.tokenize(text), ch_tokenizer.span_tokenize(text)))\n",
    "sections = []\n",
    "\n",
    "for i, section in enumerate(sections_by_title_indices):\n",
    "    c = [section[0], [section[1][1], 0]]\n",
    "    try:\n",
    "        c[1][1] = sections_by_title_indices[i+1][1][0]\n",
    "    except:\n",
    "        pass\n",
    "    sections.append(c)\n",
    "\n",
    "section_indices = []\n",
    "i = 0\n",
    "for section in sections:\n",
    "    # Skip the chapter title and final notes section.\n",
    "    if 'INDUSTRIAL SOCIETY AND ITS FUTURE' in section[0] or 'NOTES' in section[0]:\n",
    "        continue\n",
    "    else:\n",
    "        entry = {\n",
    "            'section': section[0],\n",
    "            'index_left': section[1][0],\n",
    "            'index_right': section[1][1]\n",
    "        }\n",
    "        section_indices.append(entry)\n",
    "        i = 1\n",
    "\n",
    "df = pd.DataFrame(section_indices)\n",
    "df.to_csv('section_list.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above simply maps all of the headings and then uses them to figure out where each section begins and ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Paragraph Indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "punctuation = string.punctuation + '“”’.,'\n",
    "text = open('manifesto.txt').read()\n",
    "\n",
    "pp_tokenizer = RegexpTokenizer('^(?:[0-9]+ ?)+?\\.\\s')\n",
    "paragraphs_by_title_indices = list(zip(pp_tokenizer.tokenize(text), pp_tokenizer.span_tokenize(text)))\n",
    "paragraphs = []\n",
    "\n",
    "for i, paragraph in enumerate(paragraphs_by_title_indices):\n",
    "    p = [paragraph[0], [paragraph[1][1], 0]]\n",
    "    try:\n",
    "        p[1][1] = paragraphs_by_title_indices[i+1][1][0]\n",
    "    except:\n",
    "        pass\n",
    "    paragraphs.append(p)\n",
    "\n",
    "paragraph_indices = []\n",
    "\n",
    "# Skip Notes at end of doc.\n",
    "break_next = False\n",
    "i = 0\n",
    "for paragraph in paragraphs:\n",
    "    if break_next:\n",
    "        break\n",
    "\n",
    "    # we want to skip the ones after 232 because they appear in the \"final notes\" section which I cut.\n",
    "    if '232. ' in paragraph[0]:\n",
    "        break_next = True\n",
    "\n",
    "    entry = {\n",
    "        'paragraph': paragraph[0].replace('. ', ''),\n",
    "        'index_left': paragraph[1][0],\n",
    "        'index_right': paragraph[1][1]\n",
    "    }\n",
    "    paragraph_indices.append(entry)\n",
    "    i = 1\n",
    "\n",
    "print(paragraph_indices)\n",
    "\n",
    "df = pd.DataFrame(paragraph_indices)\n",
    "df.to_csv('paragraph_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the reference in the writeup on section generation, the code above simply maps the indicies of every numbered paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Most Frequent Word Indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "punctuation = string.punctuation + '“”’.,'\n",
    "text = open('manifesto.txt').read()\n",
    "\n",
    "most_common_words = {}\n",
    "for row in csv.DictReader(open('most_common_words.csv')):\n",
    "    most_common_words[row['word']] = {\n",
    "        'rank': row['rank'],\n",
    "        'count': row['count']\n",
    "    }\n",
    "\n",
    "word_indices = []\n",
    "\n",
    "# https://stackoverflow.com/a/14307628/6669540\n",
    "def all_occurences(file, str):\n",
    "    initial = 0\n",
    "    while True:\n",
    "        initial = file.find(str, initial)\n",
    "        if initial == -1: return\n",
    "        yield initial\n",
    "        initial += len(str)\n",
    "\n",
    "for common in most_common_words:\n",
    "    indicies = all_occurences(text, common)\n",
    "    for index in indicies:\n",
    "        entry = {\n",
    "            'word': common,\n",
    "            'index': index,\n",
    "            'rank': most_common_words[common]['rank']\n",
    "        }\n",
    "\n",
    "        word_indices.append(entry)\n",
    "\n",
    "print(word_indices)\n",
    "\n",
    "df = pd.DataFrame(word_indices)\n",
    "df.to_csv('word_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "This proved to be a great project for me to work on. It pushed me both in creative thinking abilities (in trying to come up with a visualization) but also in my text-anlysis abilities, which admittedly are still rather limited. Overall, I am glad that I had the opportunity to work on such a project. Make sure to check it out here: http://csc630.rudd.io/projects/unabomber-manifesto/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations and Attributions\n",
    "\n",
    "- \"The Unabomber Trial: The Manifesto.\" *The Washington Post* http://www.washingtonpost.com/wp-srv/national/longterm/unabomber/manifesto.text.htm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
