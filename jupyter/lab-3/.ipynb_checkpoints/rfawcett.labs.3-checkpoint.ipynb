{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "## Lab 3: Acquiring and Processing Data\n",
    "\n",
    "### Name: Rudd Fawcett\n",
    "\n",
    "**Directions:** Throughout this notebook, add markdown cells as needed in order to document your process of trying to understand it.  I don't expect to just see three perfect answers and no explanations.  Tell me what you are trying!  In the end, make sure your cell and kernel linearities line up (that is, make sure that your final product can be followed from top to bottom).\n",
    "\n",
    "Our goal is to gain a bit more familiarity with the way you might generate datasets from APIs or from a larger CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Maps Geocoding API\n",
    "\n",
    "Below is a function that generates a random Latitude and Longitude in Wyoming (it's a [particularly square state](https://www.mapsofworld.com/usa/states/wyoming/wyoming-maps/wyoming-lat-long-map.jpg)).  **Your first goal** is to use the Google Maps Geocoding API to create a dataset of 10 random locations in Wyoming and the town and zip code they lie in.  For example:\n",
    "```\n",
    "[{\"lat\": \"44.952055\", \"lon\": \"-107.67753\", \"town\": \"Parkman\", \"zip\": \"82838\"}, ...]\n",
    "```\n",
    "\n",
    "To do this: \n",
    "\n",
    "1. [Get a google maps geocoding API key](https://developers.google.com/maps/documentation/geocoding/get-api-key) -- It's free and quick, just tell them the name of your \"app\" (it can be anything)\n",
    "2. [Take a look at how to use the geocoding API](https://developers.google.com/maps/documentation/geocoding/start) -- You're looking for the process they call \"reverse geocoding\"\n",
    "3. Build your request\n",
    "4. Interpret the result: it's a JSON response with tons of extra data so let me help you: you'll need `your_request_response_object.json()`.  Just grab the first result's `address_components` and dive into that array, or look for the results `formatted_address` and find the town and zip code in the resulting string (try `my_string.split(,)`.)\n",
    "5. Save the data as a [JSON file](https://docs.python.org/2/library/json.html) (or [use pandas](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html), possibly easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_lat_long():\n",
    "    \"\"\" Generate a random latitude and longitude in Wyoming. \n",
    "    Bounds: 41째N to 45째N and 104.05째W to 111.05째W\n",
    "    \"\"\"\n",
    "    return \"{},{}\".format(round(random.uniform(41, 45), 5), round(random.uniform(-111.05,-104.05), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyC-YrFjOAtNoqTFwdy9OXfoVp7V_TXEWts'\n",
    "\n",
    "locations = []\n",
    "\n",
    "# Loop through this 10 times\n",
    "for number in range(10):\n",
    "    location = {}\n",
    "    # Generatea a latitude and longitude\n",
    "    lat_long = generate_lat_long()\n",
    "    # String interpolation equivalent in Python\n",
    "    path = 'https://maps.googleapis.com/maps/api/geocode/json?latlng={}&key={}'.format(lat_long, API_KEY)\n",
    "    # GET request to Google API\n",
    "    response = requests.get(path)\n",
    "    # Grab relevant results\n",
    "    data = response.json()['results'][0]\n",
    "  \n",
    "    # Setting the keys and values on my location dictionary.\n",
    "    location['lat'] = data['geometry']['location']['lat']\n",
    "    location['lon'] = data['geometry']['location']['lng']\n",
    "    location['town'] = data['address_components'][1]['long_name']\n",
    "    # Using -1 in python to get last item in array, converting string to int.\n",
    "    location['zip'] = int(data['address_components'][-1]['long_name'])\n",
    "    \n",
    "    # Add the location to the array\n",
    "    locations.append(location)\n",
    "\n",
    "# Open/create a locations.json file in write mode\n",
    "with open('locations.json', 'w') as output:\n",
    "    # Dump JSON into file\n",
    "    json.dump(locations, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing a dataset using pandas\n",
    "\n",
    "[This CSV](http://introcs.cs.princeton.edu/java/data/bnc-wordfreq.csv) contains word frequencies in a subset of the British National Corpus, a 100 million long collect\n",
    "\n",
    "Questions/tasks for you:\n",
    "1. How many words are in this dataset?\n",
    "2. Construct the dataset consisting of all nouns whose frequency is greater than 20000 and which contain an \"`ag`\" in them.  Some hints: use pandas [boolean slicing](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing); pandas can help you with [string containment](https://pandas.pydata.org/pandas-docs/stable/text.html#testing-for-strings-that-match-or-contain-a-pattern), too.\n",
    "3. Construct the dataset of all the prime-number-indexed rows.  Use [`.loc`](https://pandas.pydata.org/pandas-docs/stable/indexing.html#basics)\n",
    "\n",
    "Both of the datasets may be just constructed in memory, _i.e._ no need to save them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6318\n",
      "      RANK  FREQUENCY        WORD PART OF SPEECH\n",
      "135    160      59829       again            adv\n",
      "136    169      56208     against           prep\n",
      "137    399      25340         age              n\n",
      "148    428      23497       agree              v\n",
      "3118   462      22117    language              n\n",
      "3354   470      21884  management              n\n",
      "5301   503      20586       stage              n\n",
      "      RANK  FREQUENCY          WORD PART OF SPEECH\n",
      "1     2107       4249       abandon              v\n",
      "2     5204       1110         abbey              n\n",
      "3      966      10468       ability              n\n",
      "5     6277        809      abnormal              a\n",
      "7     5085       1154     abolition              n\n",
      "11    3341       2139         above              a\n",
      "13     786      12889         above           prep\n",
      "17    4266       1504        absent              a\n",
      "19    1651       5782    absolutely            adv\n",
      "23    5655        966        absurd              a\n",
      "29    5188       1114    accelerate              v\n",
      "31     507      20373        accept              v\n",
      "37    1207       8374      accident              n\n",
      "41    5648        968    accomplish              v\n",
      "43    5613        976        accord              v\n",
      "47     536      19260       account              n\n",
      "53    5570        987  accumulation              n\n",
      "59    4521       1387       accused              a\n",
      "61    1980       4586   achievement              n\n",
      "67    3327       2150          acre              n\n",
      "71     654      15620           act              v\n",
      "73    4708       1293      activate              v\n",
      "79    5376       1051       actress              n\n",
      "83    2906       2687         adapt              v\n",
      "89    1566       6112       address              n\n",
      "97    3769       1795    administer              v\n",
      "101   5654        966    admiration              n\n",
      "103   2861       2742     admission              n\n",
      "107   4182       1547         adult              a\n",
      "109   1840       5055       advance              n\n",
      "...    ...        ...           ...            ...\n",
      "6079  5463       1020    vocational              a\n",
      "6089  5995        869       voucher              n\n",
      "6091  3057       2447    vulnerable              a\n",
      "6101  1919       4808          walk              n\n",
      "6113  1429       6756          warm              a\n",
      "6121  5458       1022       wartime              n\n",
      "6131   261      35767         water              n\n",
      "6133  3082       2415          wave              v\n",
      "6143  1623       5873       weather              n\n",
      "6151  4715       1289          weep              v\n",
      "6163  4234       1522    well-known              a\n",
      "6173  2097       4272         wheel              n\n",
      "6197   529      19542         whole              a\n",
      "6199  3238       2249        wholly            adv\n",
      "6203  5219       1105        wicked              a\n",
      "6211  4667       1310         width              n\n",
      "6217  1380       6995          will              n\n",
      "6221  3009       2506           win              n\n",
      "6229  4908       1222       winning              a\n",
      "6247  5031       1177          wolf              n\n",
      "6257  6037        860       wording              n\n",
      "6263  3923       1701       working              n\n",
      "6269   161      59094         world              n\n",
      "6271  4907       1222     worldwide            adv\n",
      "6277  5651        966         worse            adv\n",
      "6287  4257       1508         wrist              n\n",
      "6299  4242       1516            ye           pron\n",
      "6301    60     163930          year              n\n",
      "6311   255      37278         young              a\n",
      "6317  2556       3223          zone              n\n",
      "\n",
      "[823 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "word_freq = pd.read_csv('bnc-wordfreq.csv')\n",
    "\n",
    "# Get total number of lines\n",
    "count = len(word_freq)\n",
    "print(count)\n",
    "\n",
    "# Construct data set where frequency > 20000 and the word contains 'ag'\n",
    "nouns_ag = word_freq[(word_freq['FREQUENCY'] > 20000) & (word_freq['WORD'].str.contains('ag'))]\n",
    "print(nouns_ag)\n",
    "\n",
    "# Prime method taken from: https://stackoverflow.com/a/1801446/6669540\n",
    "def is_prime(n):\n",
    "    \"\"\"Returns True if n is prime.\"\"\"\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n == 3:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    if n % 3 == 0:\n",
    "        return False\n",
    "\n",
    "    i = 5\n",
    "    w = 2\n",
    "\n",
    "    while i * i <= n:\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "\n",
    "        i += w\n",
    "        w = 6 - w\n",
    "\n",
    "    return True\n",
    "\n",
    "prime_nums = []\n",
    "\n",
    "for n in range(count):\n",
    "    if is_prime(n):\n",
    "        prime_nums.append(n)\n",
    "\n",
    "\n",
    "nouns_prime_idx = word_freq.loc[prime_nums]\n",
    "print(nouns_prime_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis\n",
    "\n",
    "The task of taking text data and making it usable is tricky and can sometimes be time consuming.  Today, we'll keep it pretty simple.\n",
    "\n",
    "1. First, open the text file [`raven.txt`](https://cs.andover.edu/~nzufelt/dataviz/raven.txt), and copy its contents into a single string.\n",
    "2. Then, remove any character that isn't a letter, `\\n`, or (perhaps!) punctuation.  (hint: `\"a\" in \"cat\"` is `True` in Python, whereas `\"&\" in \"cat\"` is `False`.)\n",
    "3. `split` the text by \"sentence\" (more likely by line for this particular text file).  The \"sentences\" will become the rows of your dataset, and the occurance of certain words will be your columns.  It might help to further `split` your sentences by word.\n",
    "4. Create a dataset from this based upon whether the words `of`, `nothing`, `raven`, and/or `chamber` appear in each sentence: each entry in your dataset will be `0` (this word/column **not** in this sentence/row) or `1` (this word/column appears in this sentence/row).\n",
    "5. Output your dataset to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"raven.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Remove all non-alpha characters and spaces\n",
    "regex = re.compile('[^a-zA-Z\\s]')\n",
    "# replace all of stanza breaks with new lines \n",
    "clean_text = regex.sub('', text).replace('\\n\\n', '\\n')\n",
    "# split lines into an array of lines\n",
    "lines = clean_text.split('\\n')\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Go through the lines and grab their index and value\n",
    "for index,line in enumerate(lines):\n",
    "    row = {\n",
    "        # Make line standard line number (aka 1,2,3 rather than 0,1,2,etc.)\n",
    "        'line': index+1,\n",
    "        # If \"of\" occurs in the line, set it to 1\n",
    "        'of_occurs': 1 if 'of' in line else 0,\n",
    "        'nothing_occurs': 1 if 'nothing' in line else 0,\n",
    "        'raven_occurs': 1 if 'raven' in line else 0,\n",
    "        'chamber_occurs': 1 if 'chamber' in line else 0\n",
    "    }\n",
    "    \n",
    "    # Add the row to rows\n",
    "    rows.append(row)\n",
    "\n",
    "text_data = pd.DataFrame(rows)\n",
    "# Set the CSV index to be line rather than Panda's default one\n",
    "text_data.set_index('line', inplace=True)\n",
    "# Dump data frame to CSV file\n",
    "text_data.to_csv('raven-text-analysis.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
